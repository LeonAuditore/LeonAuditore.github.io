---
layout:     post
title:      【转载】计算机架构，黄金十年爆发！
subtitle:   转载文章
date:       2021-3-1
author:     Write BY David Patterson && John Hennessy Translate by 新智元
header-img: post_image/jiagou_1.jpeg
catalog: true
tags:
    - 转载
    - 计算机系统结构
    - 计算机科学与技术
---
>> 这篇文章为我的计算机结构老师极力推荐的文章

图灵奖得主、计算机体系结构宗师David Patterson与John Hennessy认为，未来将是计算机体系结构的黄金十年。最新一期的ACM通讯上刊登了两人合著的论文《计算机体系结构的新黄金时代》，阐述计算性能如何实现再一次飞升。

# 计算机体系结构将迎来一个新的黄金时代！

2017年3月，计算机架构领域两位巨星级人物David Patterson与John Hennessy在斯坦福大学发表演讲时如是说。

![picture1](http://5b0988e595225.cdn.sohucs.com/images/20190131/aff08a0699a4415293911775769f9074.jpeg)

John Hennessy(左) 和David Patterson 拿着他们合著的《计算机体系架构：量化研究方法》，照片的拍摄时间大约是1991年。来源：ACM

当时，Hennessy最知名的title是斯坦福大学前任校长，而Patterson则是伯克利的退休教授。他们1990年合著出版的《计算机体系架构：量化研究方法》被誉为领域“体系结构圣经”，培养和指导了无数处理器设计人才。

当时，由GPU推动的深度学习浪潮已然兴起，谷歌推出了TPU，AI芯片创业公司林立，芯片市场群雄并起，连做软件的人都能感到从硬件行业迸发出的热气。

很快，一年后，已经出任谷歌母公司Alphabet董事长的Hennessy和已经加入谷歌TPU团队的Patterson又站在了一起，这次是为了纪念他们共同荣获2017年的图灵奖。

他们的图灵奖演讲题目叫做《计算机体系结构的新黄金时代》(A New Golden Age for computer Architecture)，两人回顾了自20世纪60年代以来计算机体系结构发展历史，并展望人工智能为计算机架构设计所带来的新的挑战和机遇。

在那次的图灵演讲中，David Patterson与John Hennessy还提到了软件设计也能为计算机硬件架构带来灵感，改善软硬件接口能为架构创新带来机遇。“在摩尔定律走向终点的同时，体系结构正在闪耀新的活力——以TPU为代表的领域特定架构 (Domain Specific Architectures, DSA) 兴起，但CPU、GPU、FPGA仍然有用武之地，最终，市场会决定胜者。

2019年2月出版的Communications of the ACM，刊登了两人的署名文章“A New Golden Age for computer Architecture”，在图灵演讲的基础之上进一步完善思想，并用文字将他们的洞见更加清晰地呈现。

![picture2](http://5b0988e595225.cdn.sohucs.com/images/20190131/1f214b978b534a4e9bd87eb45dec86ba.jpeg)

“计算机体系结构领域将迎来又一个黄金十年，就像20世纪80年代我们做研究那时一样，新的架构设计将会带来更低的成本，更优的能耗、安全和性能。”

     以下为译文：

         不能牢记过去的人，必定重蹈覆辙。

             ——George Santayana，1905年

软件通过称为指令集架构（ISA）的词汇表与硬件实现交互。在20世纪60年代初，IBM共推出了四个彼此不兼容的计算机系列，每个计算机系列都有自己的ISA、软件堆栈和输入/输出系统，分别针对小型企业、大型企业，科研单位和实时运算。 IBM的工程师们，包括ACM图灵奖获奖者Fred Brooks在内，都认为能够创建一套新的ISA，将这四套ISA有效统一起来。

这需要一套技术解决方案，让计算8位数据路径的廉价计算机和计算64位数据路径的昂贵计算机可以共享一个ISA。数据路径（data path）是处理器的“肌肉”，因为这部分负责执行算法，但相对容易“加宽”或“缩小”。当时和现在计算机设计人员共同面临的最大挑战是处理器的“大脑”，即控制硬件。受软件编程的启发，计算机先驱人物、图灵奖获得者莫里斯·威尔克斯提出了简化控制流程的思路。控制部分被指定为一个二维数组，他称之为“控制存储”。数组的每一列对应一条控制线，每一行都是微指令，写微指令的操作称为微编程。控制存储包含使用微指令编写的ISA解释器，因此执行一个传统指令需要多个微指令完成。控制存储通过内存实现，这比使用逻辑门的成本要低得多。

此表列出了IBM于1964年4月7日发布的新360系统的四种型号的指令集。四种型号之间。数据路径相差8倍，内存容量相差16倍，时钟速率相差近4倍，而性能相差50倍，其中最昂贵的机型M65具有最大空间的控制存储，因为更复杂的数据路径需要使用更多的控制线。由于硬件更简单，成本最低的机型M30的控制存储空间最小，但由于它们需要更多的时钟周期来执行360系统指令，因此需要有更多的微指令。

通过微程序设计，IBM认为新的ISA将能够彻底改变计算行业，赢得未来。 IBM统治了当时的计算机市场，55年前诞生的IBM大型机，其后代产品现在每年仍能为IBM带来100亿美元的收入。

现在看来，尽管市场对技术问题做出的评判还不够完善，但由于硬件系统架构与商用计算机之间的密切联系，市场最终成为计算机架构创新的是否成功的关键性因素，这些创新往往需要工程人员方面的大量投入。

集成电路，CISC，432,8086，IBM PC。当计算机开始使用集成电路时，摩尔定律意味着控制存储可能变得更大。更大的内存可以运行更复杂的ISA。1977年，数字设备公司（Digital Equipment）发布的VAX-11/780机型的控制存储大小达到5120 word×96 bit，而其之前的型号仅为256 word×56 bit。

一些制造商选择让选定的客户添加名为“可写控制存储”（WCS）的自定义功能来进行微程序设计。最著名WCS计算机是Alto，这是图灵奖获得者Chuck Thacker和Butler Lampson以及他们的同事们于1973年为Xerox Palo Alto研究中心设计制造的。它是第一台个人计算机，使用第一台位映射显示器和第一个以太局域网。用于支持新显示器和网络的设备控制器是存储在4096 word×32 bit WCS中的微程序。

微处理器在20世纪70年代仍处于8位时代（如英特尔的8080处理器），主要采用汇编语言编程。各家企业的设计师会不断加入新的指令来超越竞争对手，通过汇编语言展示他们的优势。

戈登·摩尔认为英特尔的下一代指令集架构将能够延续英特尔的生命，他聘请了大批聪明的计算机科学博士，并将他们送到波特兰的一个新工厂，以打造下一个伟大的指令集架构。英特尔最初推出8800处理器是一个雄心勃勃的计算机架构项目，适用于任何时代，它具有32位寻址能力、面向对象的体系结构，可变位的长指令，以及用当时新的编程语言Ada编写的自己的操作系统。

![picture3](http://5b0988e595225.cdn.sohucs.com/images/20190131/c8543ffd639a4ef59ad4b09a13e027ce.jpeg)
图1 IBM 360系列机型的参数，IPS意为“每秒操作数”

这个雄心勃勃的项目迟迟不能退出，这迫使英特尔紧急改变计划，于1979年推出一款16位微处理器。英特尔为新团队提供了52周的时间来开发新的“8086”指令集，并设计和构建芯片。由于时间紧迫，设计ISA部分仅仅花了3周时间，主要是将8位寄存器和8080的指令集扩展到了16位。团队最终按计划完成了8086的设计，但产品发布后几乎没有大张旗鼓的宣传。

英特尔很走运，当时IBM正在开发一款旨在与Apple II竞争的个人计算机，正需要16位微处理器。 IBM一度对摩托罗拉的68000型感兴趣，它拥有类似于IBM 360的指令集架构，但与IBM激进的方案相比显得落后。IBM转而使用英特尔8086的8位总线版本处理器。IBM于1981年8月12日宣布推出该机型，预计到1986年能够卖出25万台，结果最终在全球卖出了1亿台，未来前景一片光明。

英特尔的8800项目更名为iAPX-432，最终于1981年发布，但它需要多个芯片，并且存在严重的性能问题。该项目在1986年终止，此前一年，英特尔将寄存器从16位扩展到32位，在80386芯片中扩展了8086指令集架构。摩尔的预测是正确的，这个指令集确实和英特尔一直存续下来，但市场却选择了紧急赶工的产品8086，而不是英特尔寄予厚望的iAPX-432，这对摩托罗拉68000和iAPX-432的架构师来讲，都是个现实的教训，市场永远是没有耐心的。

从复杂指令集计算机到精简指令集计算机。 20世纪80年代初期，对使用大型控制存储中的大型微程序的复杂指令集计算机（CISC）的相关问题进行过几项调查。Unix的广泛应用，证明连操作系统都可以使用高级语言，所以关键问题就是：“编译器会产生什么指令？”而不是“程序员使用什么汇编语言？”软硬件交互手段的显著进步，为架构创新创造了机会。

图灵奖获得者John Cocke和他的同事为小型计算机开发了更简单的指令集架构和编译器。作为一项实验，他们重新定位了研究编译器，只使用简单的寄存器-寄存器操作和IBM 360指令集加载存储数据传输，避免了使用更复杂的指令。他们发现，使用简单子集的程序运行速度提高了三倍。 Emer和Clark发现，20％的VAX指令需要60％的微代码，仅占执行时间的0.2％。Patterson发现，如果微处理器制造商要遵循大型计算机的CISC指令集设计，就需要一种方法来修复微代码错误。

Patterson就此问题写了一篇论文，但被《计算机》期刊拒稿。审稿人认为，构建具有ISA的微处理器是一个糟糕的想法，因为这需要在现场进行修复。这让人怀疑，CISC 指令集对微处理器的价值究竟有多大。具有讽刺意味的是，现代CISC微处理器确实包含微代码修复机制，但这篇论文被拒的主要结果是，激励了他开始研究面向微处理器的精简指令集，即复杂度较低的指令集架构，以及使用精简指令集的计算机（RISC）。

这些观点的产生，以及由汇编语言向高级语言的转变，为CISC向RISC的过渡创造了条件。首先，RISC指令经过简化，因此不再需要微代码解释器。 RISC指令通常与微指令一样简单，硬件能够直接执行。其次，以前用于CISC 指令集的微代码解释器的快速存储器被重新用作RISC指令的高速缓存。（缓存是一个小而快速的内存，用于缓冲最近执行的指令，因为这类指令很快就会被再次调用。）第三，基于Gregory Chaitin的图着色方案的寄存器分配器，使编译器能够更简易、高效地使用寄存器，最后，摩尔定律意味着在20世纪80年代能够诞生有足够数量的晶体管的芯片，可以容纳一个完整的32位数据路径、指令集和数据高速缓存。

在今天的“后PC时代”，x86芯片的出货量自2011年达到峰值以来，每年下降近10％，而采用RISC处理器的芯片出货量则飙升至200亿。

下图分别为1982年和1983年在加州大学伯克利分校和斯坦福大学开发的RISC-I8和MIPS12微处理器，体现出了RISC的优点。这些芯片最终于1984年在IEEE国际固态电路会议上发表。这是一个了不起的时刻，伯克利和斯坦福的一些研究生也可以构建微处理器了，可以说比行业内的产品更优秀。

![picture4](http://5b0988e595225.cdn.sohucs.com/images/20190131/38187b81a84949a18fd165ea972f8dee.jpeg)
图2 由加州大学伯克利分校开发的RISC-I8和斯坦福大学开发的MIPS12微处理器

这些由学术机构开发的芯片，激励了许多企业开始发力RISC微处理器，并成为此后15年中发展最快的领域。其原因是处理器的性能公式：

时间/程序=操作数/程序x（时钟周期）/指令x时间/（时钟周期）


DEC公司的工程师后来表明，更复杂的CISC指令集每个程序执行的操作数大约为RISC的75%执行大约75％，不过在类似的技术下，CISC每个指令执行时间约为五到六个时钟周期，使RISC微处理器的运算速度是CISC的大约4倍。

20世纪80年代时，这些内容还没有进入计算机体系结构的书中，所以我们在1989年编写《计算机架构：定量方法》（ Computer Architecture: AQuantitative Approach）一书。本书的主题是使用测量和基准测试来对计算机架构进行量化评估，而不是更多地依赖于架构师的直觉和经验，就像过去一样。我们使用的定量方法也得益于图灵奖得主高德纳（Donald Knuth）关于算法的著作内容的启发。

VLIW，EPIC，Itanium。指令集架构的下一次创新试图同时惠及RISC和CISC，即超长指令字（VLIW）和显式并行指令计算机（EPIC）的诞生。这两项发明由英特尔和惠普共同命名的，在每条指令中使用捆绑在一起的多个独立操作的宽指令。VLIW和EPIC的拥护者认为，如果用一条指令可以指定六个独立的操作——两次数据传输，两次整数操作和两次浮点操作，编译器技术可以有效地将这些操作分配到六个指令槽中，硬件架构就可以变得更简单。与RISC方法一样，VLIW和EPIC的目的是将工作负载从硬件转移到编译器上。

英特尔和惠普合作设计了一款基于EPIC理念的64位处理器Itanium，想用其取代32位x86处理器并对其抱了很高的期望，但实际情况与他们的早期预期并不相符。虽然EPIC方法适用于高度结构化的浮点程序，但在可预测性较低的缓存或的分支整数程序上很难实现高性能。正如高德纳后来所指出的那样：“Itanium的设想非常棒，但事实证明满足这种设想的编译器基本上不可能写出来。” 开发人员注意到Itanium的迟钝和性能不佳，将用命途多舛的游轮“Titanic”其重命名为“Itanic”。不过，市场再次失去了耐心，最终64位版本的x86成为32位x86的继承者，没有轮到Itanium。

不过一个好消息是，VLIW在较窄的应用程序与小程序上，包括数字信号处理任务中留有一席之地。

# PC和后PC时代的RISC vs. CISC

AMD和英特尔利用500人的设计团队和先进的半导体技术，缩小了x86和RISC之间的性能差距。同样，受到流水线化简单指令vs.复杂指令的性能优势的启发，指令解码器在运行中将复杂的x86指令转换成类似RISC的内部微指令。AMD和英特尔随后将RISC微指令的执行流程化。RISC的设计人员为了性能所提出的任何想法，例如隔离指令和数据缓存、片上二级缓存、deep pipelines以及同时获取和执行多条指令等，都可以集成到x86中。在2011年PC时代的巅峰时期，AMD和英特尔每年大约出货3.5亿台x86微处理器。PC行业的高产量和低利润率也意味着价格低于RISC计算机。

考虑到全球每年售出数亿台个人电脑，PC软件成为了一个巨大的市场。尽管Unix市场的软件供应商会为不同的商业RISC ISA (Alpha、HP-PA、MIPS、Power和SPARC)提供不同的软件版本，但PC市场只有一个ISA，因此软件开发人员发布的“压缩打包”软件只能与x86 ISA兼容。更大的软件基础、相似的性能和更低的价格使得x86在2000年之前同时统治了台式机和小型服务器市场。

Apple公司在2007年推出了iPhone，开创了后PC时代。智能手机公司不再购买微处理器，而是使用其他公司的设计(包括ARM的RISC处理器)，在芯片上构建自己的系统(SoC)。移动设备的设计者不仅重视性能，而且重视晶格面积和能源效率，这对CISC ISA不利。此外，物联网的到来大大增加了处理器的数量，以及在晶格大小、功率、成本和性能方面所需的权衡。这种趋势增加了设计时间和成本的重要性，进一步不利于CISC处理器。在今天的后PC时代，x86的出货量自2011年达到峰值以来每年下降近10%，而采用RISC处理器的芯片的出货量则飙升至200亿。今天，99%的32位和64位处理器都是RISC。

总结上面的历史回顾，我们可以说市场已经解决了RISC-CISC的争论；CISC赢得了PC时代的后期阶段，但RISC正在赢得整个后PC时代。几十年来，没有出现新的CISC ISA。令我们吃惊的是，今天在通用处理器的最佳ISA原则方面的共识仍然是RISC，尽管距离它们的推出已经过去35年了。

处理器架构当前的挑战

虽然上一节的重点是指令集体系结构(ISA)的设计，但大多数计算机架构师并不设计新的ISA，而是在当前的实现技术中实现现有的ISA。自20世纪70年代末以来，技术的选择一直是基于金属氧化物半导体(MOS)的集成电路，首先是n型金属氧化物半导体(nMOS)，然后是互补金属氧化物半导体(CMOS)。MOS技术惊人的改进速度(Gordon Moore的预测中已经提到这一点)已经成为驱动因素，使架构师能够设计更积极的方法来实现给定ISA的性能。摩尔在1965年26年的最初预测要求晶体管密度每年翻一番;1975年，他对其进行了修订，预计每两年翻一番。这最终被称为摩尔定律。由于晶体管密度呈二次增长，而速度呈线性增长，架构师们使用了更多的晶体管来提高性能。

# 摩尔定律和 Dennard Scaling的终结

尽管摩尔定律已经存在了几十年(见图3)，但它在2000年左右开始放缓，到2018年，摩尔的预测与目前的能力之间的差距大约是15倍。目前的预期是，随着CMOS技术接近基本极限，差距将继续扩大。

![picture5](http://5b0988e595225.cdn.sohucs.com/images/20190131/05f2db08f6e74e419ce6576e55b69329.jpeg)
图3：每片英特尔微处理器的晶体管与摩尔定律的差别

与摩尔定律相伴而来的是罗伯特·登纳德(Robert Dennard)的预测，称为“登纳德缩放比例”(Dennard Scaling)。该定律指出，随着晶体管密度的增加，每个晶体管的功耗会下降，因此每平方毫米硅的功耗几乎是恒定的。由于硅的计算能力随着每一代新技术的发展而提高，计算机将变得更加节能。Dennard Scaling在2007年开始显著放缓，到2012年几乎变为零（见图4）。

![picture6](http://5b0988e595225.cdn.sohucs.com/images/20190131/23aac728bdd34481ad8ebd039d1be595.jpeg)
图4：Transistors per chip and power per mm2.

在1986年到2002年之间，指令级并行(ILP)的开发是提高性能的主要架构方法，并且随着晶体管速度的提高，每年的性能增长大约50%。Dennard Scaling的结束意味着架构师必须找到更有效的方法来利用并行性。

要理解为什么ILP的增加会导致更大的效率低下，可以考虑一个像ARM、Intel和AMD这样的现代处理器核心。假设它有一个15-stage的pipeline，每个时钟周期可以发出四条指令。因此，它在任何时刻都有多达60条指令在pipeline中，包括大约15个分支，因为它们代表了大约25%的执行指令。为了使pipeline保持完整，需要预测分支，并推测地将代码放入pipeline中以便执行。投机性的使用是ILP性能和效率低下的根源。当分支预测完美时，推测可以提高性能，但几乎不需要额外的能源成本——甚至可以节省能源——但是当它“错误地预测”分支时，处理器必须扔掉错误推测的指令，它们的计算工作和能量就被浪费了。处理器的内部状态也必须恢复到错误预测分支之前的状态，这将花费额外的时间和精力。

要了解这种设计的挑战性，请考虑正确预测15个分支的结果的难度。如果处理器架构师希望将浪费的工作限制在10%的时间内，那么处理器必须在99.3%的时间内正确预测每个分支。很少有通用程序具有能够如此准确预测的分支。

为了理解这些浪费的工作是如何累加起来的，请考虑下图中的数据，其中显示了有效执行但由于处理器的错误推测而被浪费的指令的部分。在Intel Core i7上，这些基准测试平均浪费了19%的指令。

！[picture7](http://5b0988e595225.cdn.sohucs.com/images/20190131/0495de6d83c143b7b9725717be634fdb.jpeg)
图5

但是，浪费的能量更大，因为处理器在推测错误时必须使用额外的能量来恢复状态。这样的度量导致许多人得出结论，架构师需要一种不同的方法来实现性能改进。多核时代就这样诞生了。

多核将识别并行性和决定如何利用并行性的责任转移给程序员和语言系统。多核并不能解决由于登纳德缩放比例定律结束而加剧的节能计算的挑战。无论有源堆芯对计算的贡献是否有效，有源堆芯都会消耗能量。

一个主要的障碍是Amdahl定律，它指出并行计算机的加速受到连续计算部分的限制。 为了理解这一观察的重要性，请考虑下图，其中显示了假设串行执行的不同部分(其中只有一个处理器处于活动状态)，在最多64个内核的情况下，应用程序的运行速度要比单个内核快得多。例如，当只有1%的时间是串行的，64处理器配置的加速大约是35。不幸的是，所需的功率与64个处理器成正比，因此大约45%的能量被浪费了。

![picture8](http://5b0988e595225.cdn.sohucs.com/images/20190131/361ef09a18934120a96f5f55a1a0e84b.jpeg)
图6

当然，真正的程序有更复杂的结构，其中部分允许在任何给定的时间点使用不同数量的处理器。尽管如此，定期通信和同步的需求意味着大多数应用程序的某些部分只能有效地使用一部分处理器。尽管Amdahl定律已有50多年的历史，但它仍然是一个困难的障碍。

随着Dennards Scaling定律的结束，芯片上内核数量的增加意味着功率也在以几乎相同的速度增长。不幸的是，进入处理器的能量也必须以热量的形式被移除。因此，多核处理器受到热耗散功率(TDP)的限制，即封装和冷却系统可以移除的平均功率。虽然一些高端数据中心可能会使用更先进的软件包和冷却技术，但没有电脑用户愿意在办公桌上安装一个小型热交换器，或者在背上安装散热器来冷却手机。TDP的限制直接导致了“暗硅”时代，处理器会降低时钟速率，关闭空闲内核以防止过热。另一种看待这种方法的方法是，一些芯片可以重新分配他们宝贵的权力，从空闲的核心到活跃的。

一个没有Dennards Scaling，摩尔定律减速、Amdahl法则完全有效的时代，意味着低效率限制了性能的提升，每年只有几个百分点的提升(见图6)。实现更高的性能改进需要新的架构方法，更有效地使用集成电路功能。在讨论了现代计算机的另一个主要缺点后，我们将回到可能起作用的方法上来。

![picture9](http://5b0988e595225.cdn.sohucs.com/images/20190131/0eede1a181184730851cacab5295feda.jpeg)
图7

# 被忽视的安全问题

20世纪70年代，处理器架构师将重点放在通过保护环等概念来增强计算机安全性上。这些架构师充分认识到大多数错误将出现在软件中，但是他们相信架构支持可以提供帮助。这些特性在很大程度上没有被操作系统所采用，这些操作系统被有意地集中在所谓的良性环境中(比如个人电脑)，并且成本很高，所以很快被放弃了。在软件社区中，许多人认为正式验证机制和微内核这样的技术，将为构建高度安全的软件提供有效的机制。遗憾的是，规模化软件系统和对性能的追求，使得这些技术无法跟上处理器的性能。其结果是，大型软件系统仍然存在许多安全缺陷，由于在线个人信息的大量增加，以及云计算的大规模应用，这种缺陷的影响被进一步放大了。

尽管计算机架构师们安全性的重要意识方面进展缓慢，但他们也已经开始为虚拟机和加密硬件提供安全支持。遗憾的是，这也可能为不少处理器带来了一个未知、但重要的安全缺陷。尤其是，Meltdown和Spectre安全缺陷导致了新的漏洞，这些漏洞会利用微架构中的漏洞，使得本来受保护的信息迅速泄露。Meltdown和Spectre使用所谓的侧通道攻击（Side-channel attacks），通过观察任务所需时间，将ISA级别上不可见的信息转换为时间上可见的属性，从而泄露信息。

2018年，研究人员展示了如何利用Spectre的变种在网络上泄露信息，而攻击者并不需要将代码加载到目标处理器上。尽管这种被称为NetSpectre的攻击泄漏信息速度很慢，但它可以让同一局域网(或云中的同一集群)上的任何机器受到攻击，这又产生了许多新的漏洞。随后有报告称发现了虚拟机体系结构中的另外两个漏洞。其中一种被称为“预见”(hadow)，可以渗透英特尔旨在保护高风险数据(如加密密钥)的安全机制。此后每月都有新的漏洞被发现。

侧通道攻击并不新鲜，但在早期，软件缺陷导致这种攻击往往能够成功。在Meltdown和Spectre和其他攻击中，导致受保护信息泄露的是硬件实现中的一个缺陷。处理器架构师在如何定义ISA的正确实现上存在基本的困难，因为标准定义中并没有说明执行指令序列对性能的影响，只是说明了执行指令的ISA可见的体系结构状态。处理器架构师们需要重新考虑ISA的正确实现的定义，以防范此类安全缺陷。同时，架构师们应该重新考虑他们对计算机安全性的关注程度，以及如何与软件设计人员合作来打造更安全的系统。从目前来看，架构师过于依赖于信息系统，并不愿意将安全性问题视为设计时的首要关注焦点。

# 计算机体系结构新机遇

“我们面前的一些令人叹为观止的机会被伪装成不可解决的问题。” ——John Gardner，1965年

通用处理器固有的低效率，无论是由ILP技术还是多核所致，加上登纳德缩放定律(Dennard Scaling)和摩尔定律的终结，使我们认为处理器架构师和设计人员在一般情况下难以继续保持通用处理器性能实现显著的提升。而鉴于提高处理器性能以实现新的软件功能的重要性，我们必须要问：还有哪些其他可行的方法？

有两个很清楚的机会，以及将两者结合起来所创造出的第三个机会。首先，现有的软件构建技术广泛使用具有动态类型和存储管理的高级语言。不幸的是，这些语言的可解释性和执行效率往往非常低。Leiserson等人用矩阵乘法运算为例来说明这种低效率。如图7所示，简单地把动态高级语言Python重写为C语言代码，将性能提高了47倍。使用多核并行循环处理将性能提升了大约7倍。优化内存布局提高缓存利用率，将性能提升了20倍，最后，使用硬件扩展来执行单指令多数据（SIMD）并行操作每条指令能够执行16个32位操作，让性能提高了9倍。总而言之，与原始Python版本相比，最终的高度优化版在多核英特尔处理器上的运行速度提高了62,000倍。这当然只是一个小的例子，程序员应该使用优化后的代码库。虽然它夸大了通常的性能差距，但有许多软件都可以实现这样性能100倍到1000倍的提升。

！[picture10](http://5b0988e595225.cdn.sohucs.com/images/20190131/a049434a2ac34557bd5c6d4ba0c8b70f.jpeg)

一个有趣的研究方向是关于是否可以使用新的编译器技术来缩小某些性能差距，这可能有助于体系结构的增强。尽管高效编译和实现Python等高级脚本语言的难度很大，但潜在的收益也是巨大的。哪怕编译性能提升25％的潜在收益都可能使Python程序运行速度提高数十乃至数百倍。这个简单的例子说明了在注重程序员效率的现代语言与传统方法之间有多大的差距。

领域特定结构(DAS)。一个更加以硬件为中心的方法，是设计针对特定问题域定制的体系结构，并为该领域提供显著的性能（和能效）增益，这也被称之为“领域特定结构”（DSA），是一种为特定领域可编程且通常是图灵完整的，但针对特定应用程序类别进行了定制。从这个意义上说，DSA与专用集成电路（ASIC）不同，后者通常用于单一功能，代码很少发生变化。DSA通常称为加速器，因为与在通用CPU上执行整个应用程序相比，它们只会加速某些应用程序。此外，DSA可以实现更好的性能，因为它们更贴近应用程序的需求；DSA的例子包括图形处理单元（GPU），用于深度学习的神经网络处理器和用于软件定义网络（SDN）的处理器。

DSA可以实现更好的性能和更高的能效，主要有以下四个原因：

首先也是最重要的一点，DSA利用了特定领域中更有效的并行形式。例如，单指令多数据并行（SIMD）比多指令多数据（MIMD）更有效，因为它只需要获取一个指令流并且处理单元以锁步操作。虽然SIMD不如MIMD灵活，但它很适合许多DSA。DSA也可以使用VLIW方法来实现ILP，而不是推测性的无序机制。如前所述，VLIW处理器与通用代码不匹配，但由于控制机制更简单，因此对于数量有限的几个领域更为有效。特别是，大多数高端通用处理器都是乱序超标量执行，需要复杂的控制逻辑来启动和完成指令。相反，VLIW在编译时会进行必要的分析和调度，这非常适用于运行显式并行程序。

其次，DSA可以更高效地利用内存层次结构。正如Horowitz所指出的，内存访问比加减计算成本要高得多。例如，访问32K字节缓存块所需的能量成本比32位整数相加高约200倍。这种巨大的差异使得优化存储器访问对于实现高能效至关重要。通用处理器在运行代码的时候，存储器访问往往表现出空间和时间局部性，但这在编译时非常难以预测。因此，CPU使用多级高速缓存来增加带宽，并隐藏相对较慢的片外DRAM的延迟。这些多级高速缓存通常消耗大约一半的处理器能量，但几乎都不需要对片外DRAM的所有访问，导致这些访问需要大约10倍于最后一级高速缓存访问的能量。

缓存有两大明显缺点：

数据集非常大时：当数据集非常大并且时间或空间位置较低时，缓存根本不能很好工作；
当缓存工作得很好时：当缓存运行良好时，位置非常高，这意味着，根据定义，大多数缓存大部分时间都处于空闲状态。
在编译时可以很好地定义和发现应用程序中的内存访问模式，这对于典型的DSL来说是正确的，程序员和编译器可以比动态分配缓存，更好地优化内存的使用。因此，DSA通常使用由软件明确控制的动态分层存储器，类似于矢量处理器的操作。对于合适的应用，用户控制的存储器可以比高速缓存消耗更少的能量。

第三，DSA可以适度使用较低的精度。通用CPU通常支持32位和64位整数和浮点（FP）数据。机器学习和图形学中的许多应用不需要计算得这样精确。例如，在深度神经网络（DNN）中，推理通常使用4位，8位或16位整数，从而提高数据和计算吞吐量。同样，对于DNN训练应用，FP很有用，但32位足够了，一般16位就行。

最后，DSA受益于以领域特定语言（DSL）编写的目标程序，这些程序可以利用更多的并行性，改进内存访问的结构和表示，并使应用程序更有效地映射到特定领域处理器。

# 领域特定语言

DSA要求将高级运算融入到体系结构里，但尝试从Python，Java，C或Fortran等通用语言中提取此类结构和信息实在太难了。领域特定语言（DSL）支持这一过程，并能有效地对DSA进行编程。例如，DSL可以使向量、密集矩阵和稀疏矩阵运算显式化，使DSL编译器能够有效地将将运算映射到处理器。常见的DSL包括矩阵运算语言Matlab，编程DNN的数据流语言TensorFlow，编程SDN的语言P4，以及用于指定高级变换的图像处理语言Halide。

使用DSL的难点在于如何保持足够的架构独立性，使得在DSL中编写的软件可以移植到不同的架构，同时还可以实现将软件映射到底层DSA的高效率。例如，XLA系统将Tensorflow编译到使用Nvidia GPU和张量处理器单元（TPU）的异构处理器。权衡DSA的可移植性以及效率是语言设计人员、编译器创建者和DSA架构师面临的一项有趣的研究挑战。

DSA TPU v1。以Google TPU v1作为DSA的一个例子，Google TPU v1旨在加速神经网络推理。TPU自2015年开始投入生产，应用范围从搜索查询到语言翻译再到图像识别，再到DeepMind的围棋程序AlphaGo和通用棋类程序AlphaZero，其目标是将深度神经网络推理的性能和能效提高10倍。

如图所示，TPU的组织架构与通用处理器完全不同。

![picture11](http://5b0988e595225.cdn.sohucs.com/images/20190131/6802c8157c194386a3e50b5a81fec53e.jpeg)

总结

我们考虑了两种不同的方法，通过提高硬件技术的使用效率来提高程序运行性能：首先，通过提高现代高级语言的编译性能；其次，通过构建领域特定体系结构，可以大大提高性能和效率。DSL是另一个如何改进支持DSA等架构创新的硬件/软件接口的例子。通过这些方法获得显著性能提升。在行业横向结构化之前，需要在跨抽象层次上垂直集成并做出设计决策，这是计算机早期工作的主要特征。在这个新时代，垂直整合变得更加重要，能够核查和进行复杂权衡及优化的团队将会受益。

这个机会已经引发了架构创新的激增，吸引了许多竞争性的架构理念：

Nvidia GPU使用许多内核，每个内核都有大型寄存器文件，许多硬件线程和缓存；
Google TPU依赖于大型二维收缩倍增器和软件控制的片上存储器；
FPGA，微软在其数据中心部署了现场可编程门阵列（FPGA），它可以根据神经网络应用进行定制；
CPU，英特尔提供的CPU具有许多内核，这些内核通过大型多级缓存和一维SIMD指令（微软使用的FPGA）以及更接近TPU而不是CPU的新型神经网络处理器得到增强。
除了这些大型企业外，还有数十家创业公司正在寻求自己的路径。为了满足不断增长的需求，体系结构设计师正在将数百到数千个此类芯片互连，形成神经网络超级计算机。

DNN架构的这种雪崩使计算机架构变得有趣。在2019年很难预测这些方向中哪些（或者即使有）会赢，但市场肯定会最终解决技术以及架构争议。

# 开放式架构

受开源软件成功的启发，计算机体系结构中的第二个机遇是开源的ISA。要创建一个“面向处理器的Linux”，该领域需要行业标准的开源ISA，这样社区就可以创建开源内核(除了拥有专有内核的个别公司之外)。如果许多组织使用相同的ISA设计处理器，那么更大的竞争可能会推动更快的创新。目标是为芯片提供处理器，成本从几美分到100美元不等。

第一个例子是RISC-V（称为“RISC Five”），这是加州大学伯克利分校开发的第五个RISC架构。RISC-V的社区在RISC-V基金会(http://riscv.org/)的管理下维护体系结构。开源允许ISA在公开的场景下发生演变，在决策最终确定之前，由硬件和软件专家进行协作。

RISC-V是一个模块化的指令集。一小部分指令运行完整的开源软件栈，然后是可选的标准扩展设计器，设计人员可以根据需求包含或省略这些扩展。这个基础包括32位和64位版本。RISC-V只能通过可选扩展来发展；即使架构师不接受新的扩展，软件堆栈仍可以很好的运行。

专有架构通常需要向上的二进制兼容性，这意味着当处理器公司添加新特性时，所有在此之后的处理器也必须包含这些新特性。而RISC-V则不是如此，所有增强功能都是可选的，如果应用程序不需要，可以删除。以下是迄今为止的标准扩展名，使用的是代表其全名的缩写：

M. Integer multiply/divide；
A. Atomic memory operations；
F/D. Single/double-precision floating-point；
C. Compressed instructions。
RISC-V的第三个显著特征是ISA的简单性。虽然难以量化，但这里有两个与ARM公司同期开发的ARMv8架构的比较：

更少的指令。RISC-V的指令要少得多。在基础版本中只有50个，在数量和性质上与最初的RIS-i惊人地相似。剩下的标准扩展(M，A，F和D)添加了53条指令，再加上C又增加了34条，共计137条。而ARMv8有500多条。
更少的指令格式。 RISC-V的指令格式少了六个，而ARMv8至少有14个。
简单性减少了设计处理器和验证硬件正确性的工作量。 由于RISC-V的目标范围从数据中心芯片到物联网设备，因此设计验证可能是开发成本的重要组成部分。

第四，RISC-V是一个全新的设计。与第一代RISC架构不同，它避免了微架构或依赖技术的特性(如延迟分支和延迟加载)，也避免了被编译器技术的进步所取代的创新(如注册窗口)。

最后，RISC-V通过为自定义加速器保留大量的操作码空间来支持DSA。

除了RISC-V之外，英伟达还(在2017年)宣布了一种名为NVDLA的免费开放架构，这是一种可伸缩、可配置的机器学习推断DSA。配置选项包括数据类型(int8、int16或fp16)和二维乘法矩阵的大小。模具尺寸从0.5 mm2到3 mm2不等，功率从20毫瓦到300毫瓦不等。ISA、软件堆栈和实现都是开放的。

开放的简单体系结构与安全性是协同的。

首先，安全专家不相信通过“隐藏”实现的安全性，因此实现开源很有吸引力，而开源的实现需要开放的体系结构。

同样重要的是，增加能够围绕安全架构进行创新的人员和组织的数量。专有架构限制了员工的参与，但是开放架构允许学术界和业界所有最优秀的“头脑”来帮助共同实现安全性。

最后，RISC-V的简单性使得它的实现更容易检查。

此外，开放的架构、实现和软件栈，加上FPGA的可塑性，意味着架构师可以在线部署和评估新的解决方案，并每周迭代它们，而不是每年迭代一次。虽然FPGA比定制芯片慢10倍，但这种性能仍然足以支持在线用户。

我们希望开放式架构成为架构师和安全专家进行硬件/软件协同设计的典范。

# 轻量级硬件开发

由Beck等人撰写的《轻量级软件开发》(The Manifesto for Agile Software Development，2011)彻底改变了软件开发方式，克服了瀑布式开发中传统的详细计划和文档的频繁失败。小型编程团队在开始下一次迭代之前快速开发工作原型(但不完整)并获得了客户反馈。轻量级开发的scrum版本汇集了5到10个程序员的团队，每次迭代执行需两到四周的冲刺。

再次受到软件成功的启发，第三个机遇是轻量级硬件开发。对于架构师来说，好消息是现代电子计算机辅助设计(ECAD)工具提高了抽象级别，从而支持轻量级开发，而这种更高的抽象级别增加了设计之间的重用。

但从设计芯片到得到用户反馈的几个月之间，像轻量级软件开发那样申请“硬件四周的冲刺”似乎是不合理的。

下图概述了轻量级开发方法如何通过在适当的级别上更改原型来工作。

![picture12](http://5b0988e595225.cdn.sohucs.com/images/20190131/3dbdd437532c44d488e6a1d7ca84b9d7.jpeg)

最内层是一个软件模拟器，如果模拟器能够满足迭代，那么这一环节是进行更改最容易和最快的地方。
下一级是FPGA，它的运行速度比一个详细的软件模拟器快数百倍。FPGA可以运行像SPEC那样的操作系统和完整的基准测试，从而可以对原型进行更精确的评估。Amazon Web Services在云中提供FPGA，因此架构师可以使用FPGA，而不需要先购买硬件并建立实验室。
为了记录芯片面积和功率的数字，下一个外层使用ECAD工具生成芯片的布局。
即使在工具运行之后，在准备生产新的处理器之前，仍然需要一些手动步骤来细化结果。处理器设计人员将此级别称为“Tape-In”。
前四个级别都支持为期四周的冲刺。

出于研究目的，当面积、能量和性能评估是非常准确的时候，我们可以停在Tape-In这个级别上。然而，这就像跑一场长跑，在终点前100码停了下来，尽管在比赛前做了大量的准备工作，运动员还是会错过真正跨越终点线的兴奋和满足。硬件工程师与软件工程师相比，其中一个优势是他们可以构建物理的东西。让芯片重新测量、运行真实的程序并显示给朋友和家人，这是硬件设计的一大乐趣。

许多研究人员认为他们必须在此环节停下来，因为制造一个芯片的成本着实太高了。当设计规模较小的时候，它们出奇的便宜：只需14,000美元即可订购100个1-mm2芯片。在28nm的规格下，1 mm2的面积包含数百万个晶体管，足够容纳一个RISC-V处理器和一个NVLDA加速器。如果设计者的目标是构建一个大型芯片，那么最外层的成本是极其昂贵的，但是架构师可以用小型芯片演示许多新颖的想法。

# 总结

“黎明前最黑暗。”

——托马斯·富勒，1650

为了从历史的教训中获益，架构师必须意识到软件创新也可以激发架构师的兴趣，提高硬件/软件界面的抽象层次可以带来创新机会，并且市场最终会解决计算机架构的争论。 iAPX-432和Itanium说明了架构投资如何超过回报，而S/360,8086和ARM提供了长达数十年的高年度回报，并且看不到尽头。

登纳德缩放比例定律和摩尔定律的终结，以及标准微处理器性能增长的减速，这些都不是必须解决的问题，而是公认的事实，并且提供了让人惊叹的机遇。

高级、特定于领域的语言和体系结构，将架构师从专有指令集的链中解放出来，以及公众对改进安全性的需求，将为计算机架构师带来一个新的黄金时代。

在开源生态系统的帮助下，轻量级开发的芯片将会令人信服，从而加速商业应用。这些芯片中通用处理器的ISA理念很可能是RISC，它经受住了时间的考验。可以期待与上一个黄金时代相同的快速改善，但这一次是在成本、能源、安全以及性能方面。

下一个十年将会是一个全新计算机架构的“寒武纪”大爆发，这意味着计算机架构师在学术界和工业界将迎来一个激动人心的时代。







